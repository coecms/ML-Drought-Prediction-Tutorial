{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ecbc9f5",
   "metadata": {},
   "source": [
    "\n",
    "<i>Last updated 31 May 2023</i>\n",
    "<h1> Random Forest: A Tutorial for Drought Prediction and Beyond </h1>\n",
    "\n",
    "In this tutorial, we will cover the following topics:\n",
    "\n",
    "1. A brief introduction to Random Forest<br>\n",
    "2. Python packages that support Random Forest<br>\n",
    "3. Modeling with Random Forest<br>\n",
    "    a. Training a Random Forest for binary classification (drought vs. non-drought)<br>\n",
    "    b. Predicting on test data using the trained model<br>\n",
    "    c. Evaluating model performance<br>\n",
    "    d. Feature importance analysis\n",
    "    e. Training with different seeds\n",
    "4. Final remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915aa338",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76ab83",
   "metadata": {},
   "source": [
    "<h2> 1. A brief introduction to Random Forest </h2>\n",
    "\n",
    "Random Forest is an ensemble learning method that can be used for both classification and regression tasks. It works by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "In the case of classification, the split at each node is determined by selecting the best split point that maximizes the Gini impurity reduction. For regression, the split is chosen to minimize the mean squared error.\n",
    "\n",
    "The main parameters that need to be tuned in a Random Forest model are:\n",
    "\n",
    "<i>n_estimators :</i> The number of trees in the forest<br>\n",
    "<i>max_depth :</i> The maximum depth of the tree<br>\n",
    "<i>min_samples_split :</i> The minimum number of samples required to split an internal node<br>\n",
    "<i>min_samples_leaf :</i> The minimum number of samples required to be at a leaf node<br>\n",
    "<i>max_features :</i> The number of features to consider when looking for the best split<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b76ab3",
   "metadata": {},
   "source": [
    "<h2> 2. Python Packages that support Random Forest </h2>\n",
    "\n",
    "Here are some popular Python packages that support Random Forest:\n",
    "\n",
    "**Scikit-learn**  - RandomForestClassifier and RandomForestRegressor (CPU only) (Installed in hh5 analysis3)<br>\n",
    "**RAPIDS**  - RandomForestClassifier and RandomForestRegressor (GPU only) (Installed in NCI dk92)<br>\n",
    "**XGBoost**  - XGBClassifier and XGBRegressor (CPU and GPU support) (Installed in NCI dk92)<br>\n",
    "**LightGBM**  - LGBMClassifier and LGBMRegressor (CPU and GPU support) <br>\n",
    "**CatBoost**  - CatBoostClassifier and CatBoostRegressor (CPU and GPU support) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bebeb8",
   "metadata": {},
   "source": [
    "<h2> 3. Modeling with Random Forest </h2>\n",
    "\n",
    "The dataset we are working with contains records of both drought and non-drought events that took place in Texas. Each event is accompanied by measurements of various climate variables. Our goal is to train a model that can accurately predict whether the climate conditions in a given month correspond to a drought or a non-drought situation.<br>\n",
    "\n",
    "This task can be framed as a binary classification problem, where the target variable is either '1' for drought or '0' for non-drought. The predictors are the climate variables.<br>\n",
    "\n",
    "For this example, we will use the **RandomForestClassifier** from the **scikit-learn** package. <br><br>\n",
    "\n",
    "First, we need to import the necessary libraries and load our dataset. We will assume that the dataset has been preprocessed and is ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7db7f6-26ea-4f51-a8bd-57f094c2b4f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "# Data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Jupyter lab widgets\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Scikit-learn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2115227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('/g/data/hh5/tmp/CLEX-WinterSchool-2023/ML-Drought-Prediction-Tutorial/Data/Texas_drought_data.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Display the first 3 lines of data\n",
    "print(data.head(3))\n",
    "\n",
    "predictors = ['P', 'PET', 'ET', 'SM', 'SM_prev', 'DS', 'NDVI', 'enso', 'month']\n",
    "target = 'drought'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb7137",
   "metadata": {},
   "source": [
    "Predictors include Precipitation <i>(**P**)</i>, <i>Evapotranspiration (**ET**)</i>, <i>Potential Evapotranspiration (**PET**)</i>, <i>Soil moisture (**SM**)</i>, <i>Soil moisture of the previous month (**SM_prev**)</i>, <i>ENSO index (**enso**)</i>, <i>**NDVI**</i>, and the <i>**month**</i> at which the event occurred.The traget variable is <i>**drought**</i>. <br>\n",
    "For more details on the data source, refer to **Table 1** in <a href=\"https://doi.org/10.1029/2021WR031829\">(Hobeichi et al., 2022)</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0cb38f",
   "metadata": {},
   "source": [
    "The following script shows the locations of the recorded drought events in drought impact reports and climate statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c8d2c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find unique points\n",
    "unique_points = data[['lon', 'lat']].drop_duplicates()\n",
    "\n",
    "# Create a GeoDataFrame from the unique locations\n",
    "geometry = [Point(xy) for xy in zip(unique_points['lon'], unique_points['lat'])]\n",
    "unique_points_gdf = gpd.GeoDataFrame(unique_points, geometry=geometry)\n",
    "\n",
    "# Load a GeoDataFrame with the geometries of the US states\n",
    "usa = gpd.read_file('/g/data/hh5/tmp/CLEX-WinterSchool-2023/ML-Drought-Prediction-Tutorial/Data/usa_boundaries/tl_2019_us_state.shp')\n",
    "texas = usa[usa['NAME'] == 'Texas']['geometry'].values[0]\n",
    "lon, lat = texas.exterior.xy\n",
    "\n",
    "\n",
    "# Plot the unique locations on a map of Texas\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(lon,lat, c='grey')\n",
    "unique_points_gdf.plot(ax=ax, color='red', markersize=5, label='Locations')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Locations of reported drought/no drought events')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf05ac",
   "metadata": {},
   "source": [
    "To check if the original dataset is balanced, we can count the number of occurrences of each class in the target variable and calculate their respective proportions. A balanced dataset will have approximately equal proportions of each class.<br><br>\n",
    "This script will display the class counts and proportions and determine if the dataset is balanced or imbalanced based on the specified threshold (10%). We can adjust the <i>**balanced_threshold**</i> variable to change the tolerance for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5748eff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictors dataframe\n",
    "X = data[predictors]\n",
    "# Display the first 3 lines\n",
    "print(\"predictors: \\n\",X.head(3))\n",
    "\n",
    "# Target data array\n",
    "y = data[target]\n",
    "# Display the first 3 lines\n",
    "print(\"\\n\\ntarget: \\n\", y.head(3))\n",
    "\n",
    "# Count the number of occurrences of each class in the target variable\n",
    "class_counts = y.value_counts()\n",
    "\n",
    "# Calculate the proportions of each class\n",
    "class_proportions = class_counts / len(y)\n",
    "\n",
    "# Display class counts and proportions\n",
    "print(\"Class counts:\\n\", class_counts)\n",
    "print(\"\\nClass proportions:\\n\", class_proportions)\n",
    "\n",
    "# Check if the dataset is balanced\n",
    "balanced_threshold = 0.1  # Define a threshold for class imbalance (e.g., 0.1 for 10%)\n",
    "is_balanced = np.all(np.abs(class_proportions - 0.5) <= balanced_threshold)\n",
    "\n",
    "if is_balanced:\n",
    "    print(\"\\nThe dataset is balanced.\")\n",
    "else:\n",
    "    print(\"\\nThe dataset is imbalanced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e232732",
   "metadata": {},
   "source": [
    "Given the balanced distribution of drought and non-drought events in our dataset, there is no need to address the issue of imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d290b8d3",
   "metadata": {},
   "source": [
    "<h3>3.a Training a Random Forest for Binary Classification (drought vs. non-drought)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f18a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "clf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff8a1b",
   "metadata": {},
   "source": [
    "It is important to specify a value for the <i>random_state</i> in order to ensure reproducible results.<br> \n",
    "Here, 30% of the data is held back for model evaluation and is not used in the training process.<br> \n",
    "The Random Forest model is built using 500 trees.<br> \n",
    "\n",
    "One advantage of using Random Forest is that it does not require extensive hyperparameter tuning or normalisation of predictors, unlike most of the other machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc496ef",
   "metadata": {},
   "source": [
    "<h3> 3.b Predicting on test data using the trained model</h3>\n",
    "Once we have trained our Random Forest model, we can use it to predict on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01821d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Display y_pred array\n",
    "print(\"Predicted classes for the test data:\\n\", y_pred)\n",
    "\n",
    "# Calculate the number of droughts and non-droughts in y_pred\n",
    "drought_count = np.sum(y_pred == 1)\n",
    "non_drought_count = np.sum(y_pred == 0)\n",
    "\n",
    "print(f\"\\nNumber of predicted droughts: {drought_count}\")\n",
    "print(f\"Number of predicted non-droughts: {non_drought_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19c2a1",
   "metadata": {},
   "source": [
    "The y_pred array contains the predicted classes (drought or non-drought) for each data point in the test dataset.<br>\n",
    "Displaying the total number of predicted droughts and non-droughts can be useful to understand the distribution of the predictions made by the model.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c0a4d3",
   "metadata": {},
   "source": [
    "The trained model can also predict the likelihood of each classification, such as the likelyhood of a drought based on the climate conditions. This can be achieved through the use of <i>**predict_proba**</i> function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bcd64b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for the test data\n",
    "y_pred_proba = clf.predict_proba(X_test)\n",
    "\n",
    "# Display y_pred_proba array\n",
    "#print(\"Predicted class probabilities for the test data:\\n\", y_pred_proba)\n",
    "\n",
    "# Extract the probability of drought (class 1) for each data point in the test dataset\n",
    "drought_probabilities = y_pred_proba[:, 1]\n",
    "\n",
    "print(\"Probability of drought for each data point in the test dataset:\\n\", drought_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c08d3-9dc6-4d99-ae02-c2a15f1bd51f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# range of each predictor\n",
    "ranges = {\n",
    "    'enso': [-4, 4],\n",
    "    'PET': [10, 250],\n",
    "    'P': [0, 700],\n",
    "    'ET': [-10, 200],\n",
    "    'SM': [0, 60],\n",
    "    'SM_prev': [0, 60],\n",
    "    'DS': [-150, 200],\n",
    "    'NDVI': [0, 1],\n",
    "    'month': [1, 12]\n",
    "}\n",
    "\n",
    "def predict_drought_probability(change):\n",
    "    # Get input values\n",
    "    input_values = [float(entry.value) for entry in entries]\n",
    "\n",
    "    # Predict the probability of drought\n",
    "    prob = clf.predict_proba([input_values])[0][1]\n",
    "\n",
    "    # Display the result\n",
    "    result_label.value = f\"Probability of drought: {prob:.2f}\"\n",
    "\n",
    "# Create input fields\n",
    "entries = []\n",
    "for idx, predictor in enumerate(predictors):\n",
    "    label = widgets.Label(value=f\"{predictor} ({ranges[predictor][0]}, {ranges[predictor][1]})\")\n",
    "    entry = widgets.FloatText(value=0, min=ranges[predictor][0], max=ranges[predictor][1])\n",
    "    entries.append(entry)\n",
    "\n",
    "    box = widgets.HBox([label, entry])\n",
    "    display(box)\n",
    "\n",
    "# Create the 'Predict' button\n",
    "predict_button = widgets.Button(description=\"Predict\")\n",
    "predict_button.on_click(predict_drought_probability)\n",
    "display(predict_button)\n",
    "\n",
    "# Create a label to display the result\n",
    "result_label = widgets.Label(value=\"\")\n",
    "display(result_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e7be6",
   "metadata": {},
   "source": [
    "Next, input your own set of predictors and see in each case the predicted probability of drought by the trained Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eff07f",
   "metadata": {},
   "source": [
    "<h3> 3.c Evaluating model performance</h3>\n",
    "\n",
    "To compute the accuracy and other performance metrics, we can use the functions provided by <i>**scikit-learn**</i>. We will calculate precision, recall, F1-score, and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dfe11c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Display performance metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
    "\n",
    "print(\"\\nPerformance Metrics Explained:\")\n",
    "print(\"Accuracy: The proportion of correct predictions (both droughts and non-droughts) out of all predictions.\")\n",
    "print(\"Precision: The proportion of true droughts out of all the predicted droughts. High precision indicates a low number of false positives (non-droughts predicted as droughts).\")\n",
    "print(\"Recall: The proportion of true droughts out of all the actual droughts. High recall indicates a low number of false negatives (droughts predicted as non-droughts).\")\n",
    "print(\"F1-score: The harmonic mean of precision and recall. It is a single metric that balances the trade-off between precision and recall.\")\n",
    "print(\"Balanced Accuracy: The average of recall obtained on each class. It is useful for imbalanced datasets, as it takes into account the performance on both minority and majority classes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cade067",
   "metadata": {},
   "source": [
    "We have to use <i>**Balanced Accuracy**</i> instead of <i>**Accuracy**</i> if the dataset is imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b9e9c7",
   "metadata": {},
   "source": [
    "<h3>3.d Feature importance analysis</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155950e",
   "metadata": {},
   "source": [
    "To calculate the variable importance of each predictor using the Random Forest model, we can access the <i>**feature_importances_ attribute**</i> of the trained model. We can then find the most important predictor and check if any of the importances are negative and conclude that the predictor doesn't provide any predictability in that case.\n",
    "Note that in practice, the feature importances provided by the <i>**RandomForestClassifier**</i> in <i>**scikit-learn**</i> are generally non-negative. However, other packages can calculate negative variable importance<br>\n",
    "\n",
    "To display the calculated variable importance in a bar graph, we can use the <i>**matplotlib**</i> library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd6ffd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate variable importance using the Random Forest model\n",
    "variable_importance = clf.feature_importances_\n",
    "\n",
    "# Check if any variable importances are negative\n",
    "negative_importance = variable_importance < 0\n",
    "\n",
    "# Display variable importances and negative importance indicators\n",
    "print(\"Variable Importances:\\n\", variable_importance)\n",
    "print(\"\\nNegative Importance Indicators:\\n\", negative_importance)\n",
    "\n",
    "# Conclude if predictors with negative importances don't provide any predictability\n",
    "if np.any(negative_importance):\n",
    "    print(\"\\nPredictors with negative importance values do not provide any predictability.\")\n",
    "else:\n",
    "    print(\"\\nNo predictors have negative importance values. All predictors provide some degree of predictability.\")\n",
    "\n",
    "\n",
    "# Create a bar graph of variable importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(X.columns, variable_importance)\n",
    "plt.xlabel('Predictors')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Variable Importance in Random Forest Model')\n",
    "plt.show()\n",
    "\n",
    "# To display the ranking of the predictors\n",
    "# Get the indices that would sort the variable importance array in descending order\n",
    "sorted_indices = np.argsort(variable_importance)[::-1]\n",
    "\n",
    "# Create a DataFrame to display predictors and their rankings\n",
    "ranking_df = pd.DataFrame({\n",
    "    'Predictor': X.columns[sorted_indices],\n",
    "    'Variable Importance': variable_importance[sorted_indices],\n",
    "    'Rank': np.arange(1, len(X.columns) + 1)\n",
    "})\n",
    "\n",
    "# Print the ranking of predictors based on variable importance\n",
    "print(\"Ranking of predictors based on variable importance:\")\n",
    "print(ranking_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8a0cf8",
   "metadata": {},
   "source": [
    "<h4> What can the feature importance analysis tell us? </h4>\n",
    "\n",
    "1. Feature selection: Variable importance can guide us in selecting the most relevant predictors for our prediction task, allowing us to build more parsimonious models with fewer predictors. This may lead to better generalization, easier interpretation, and reduced computational complexity.\n",
    "\n",
    "2. Domain knowledge: By evaluating the importance of predictors, we can validate our domain knowledge and scientific understanding of the phenomena. For example, if we find that certain predictors have a higher importance, this might confirm existing hypotheses about the role of these factors in causing droughts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae21a56",
   "metadata": {},
   "source": [
    "<h3>3.e Training with different seeds</h3>\n",
    "\n",
    "Training Random Forest with different seeds and analysing the variability of the results helps to assess the stability and generalisability of the model, which are crucial aspects of building a reliable machine learning model.<br>\n",
    "This updated script performs a new train-test split in each iteration and calculates the mean performance metrics across the 30 iterations. This approach provides an even better assessment of the model's stability and generalisability by considering the variability introduced by different splits of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b6cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_iterations = 30\n",
    "seeds = np.arange(n_iterations)\n",
    "\n",
    "# Initialize arrays to store performance metrics\n",
    "accuracy_scores = np.zeros(n_iterations)\n",
    "precision_scores = np.zeros(n_iterations)\n",
    "recall_scores = np.zeros(n_iterations)\n",
    "f1_scores = np.zeros(n_iterations)\n",
    "balanced_accuracy_scores = np.zeros(n_iterations)\n",
    "\n",
    "# Train the model and calculate performance metrics for each iteration\n",
    "for i, seed in enumerate(seeds):\n",
    "    # Split the data into training and testing sets for each iteration\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # Train the Random Forest model with a different seed\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy_scores[i] = accuracy_score(y_test, y_pred)\n",
    "    precision_scores[i] = precision_score(y_test, y_pred)\n",
    "    recall_scores[i] = recall_score(y_test, y_pred)\n",
    "    f1_scores[i] = f1_score(y_test, y_pred)\n",
    "    balanced_accuracy_scores[i] = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the mean performance metrics\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "mean_precision = np.mean(precision_scores)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_balanced_accuracy = np.mean(balanced_accuracy_scores)\n",
    "\n",
    "# Display mean performance metrics\n",
    "print(f\"Mean Accuracy: {mean_accuracy:.3f}\")\n",
    "print(f\"Mean Precision: {mean_precision:.3f}\")\n",
    "print(f\"Mean Recall: {mean_recall:.3f}\")\n",
    "print(f\"Mean F1-score: {mean_f1:.3f}\")\n",
    "print(f\"Mean Balanced Accuracy: {mean_balanced_accuracy:.3f}\")\n",
    "\n",
    "\n",
    "# Create a DataFrame to store the performance metrics for each iteration\n",
    "performance_df = pd.DataFrame({\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1-score': f1_scores,\n",
    "    'Balanced Accuracy': balanced_accuracy_scores\n",
    "})\n",
    "\n",
    "# Draw a  box and whisker plot to display the results of each performance metric across 30 iterations\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=performance_df, palette=\"pastel\")\n",
    "plt.ylabel('Score')\n",
    "plt.title('Box and Whisker Plot for Performance Metrics across 30 Iterations')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56b1b0",
   "metadata": {},
   "source": [
    "The model's performance is consistent across different seeds. This suggests that the model is less sensitive to the randomness introduced during training and is more likely to perform well on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4082c09",
   "metadata": {},
   "source": [
    "<h2> 4. Final remarks</h2>\n",
    "Through this tutorial, we learned how to apply the Random Forest algorithm to a practical problem (drought prediction) and gained insights into important aspects of model evaluation and feature importance.\n",
    "\n",
    "The concepts and techniques learned in this tutorial can be easily applied to other prediction tasks, such as predicting the probability of bushfires, extreme events, or hail occurrences, given a set of climate predictors. \n",
    "\n",
    "To apply these concepts to these new problems, we would follow similar steps:\n",
    "\n",
    "1. Collect and preprocess the dataset containing relevant climate predictors and the target variable (bushfire occurrence, extreme event occurrence, or hail occurrence). (Not covered here, but you can follow the approach described in <a href=\"https://doi.org/10.1029/2021WR031829\">(Hobeichi et al., 2022)</a> or check out the code in <a href=\"https://zenodo.org/record/7663013\">Zenodo</a>. )\n",
    "\n",
    "2. Perform the train-test split to separate the data into training and testing datasets.\n",
    "\n",
    "3. Train a Random Forest model using the training dataset.\n",
    "\n",
    "4. Predict the probabilities of the target class (e.g., bushfire, extreme event, or hail) for the test dataset using the trained model.\n",
    "\n",
    "5. Evaluate the model's performance using various performance metrics, such as accuracy, precision, recall, F1-score, and balanced accuracy.\n",
    "\n",
    "6. Assess the importance of the predictors using the Random Forest model to identify the most influential factors for the prediction task. Consider removing predictors that have limited predictiveness, and then repeat steps 2, 3, 4, and 5 to determine if the model's performance remains the same (or improves) without their inclusion.\n",
    "\n",
    "7. Test the model's stability and generalisability by training it multiple times with different seeds and train-test splits, and calculate the mean performance metrics across the iterations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb54278",
   "metadata": {},
   "source": [
    "<h4><center>--- End of Tutorial ---</center></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7f8207",
   "metadata": {},
   "source": [
    "**Credits**<br>\n",
    "Author :  Dr. Sanaa Hobeichi<sup>1</sup>,<a href=s.hobeichi@unsw.edu.au> s.hobeichi@unsw.edu.au</a><br>\n",
    "Technical Assistant : Dr. Sam Green<sup>1</sup>,<a href=sam.green@unsw.edu.au> sam.green@unsw.edu.au</a><br>\n",
    "Design Contributor :  A/Prof. Gab Abramowitz<sup>1</sup><br><br>\n",
    "<i><sup>1</sup>ARC Centre of Excellence for Climate Extremes & The University of New South Wales Sydney</i><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902274ee",
   "metadata": {},
   "source": [
    "**Disclaimer**<br>\n",
    "This tutorial was generated with the help of OpenAI's language model, ChatGPT4, and refined by the author. While every effort has been made to ensure the accuracy of the information presented, please note that the contents of this tutorial are for educational purposes only and should not be relied upon for making business, legal, or other decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-unstable]",
   "language": "python",
   "name": "conda-env-analysis3-unstable-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
